{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objet : Test opendata des données de qualité de l'air\n",
    "\n",
    "## Objectif\n",
    "\n",
    "- valider sur des cas réels l'outil de traitement des \"listes indexées\"\n",
    "- identifier les apports que pourraient avoir ce type d'outil\n",
    "\n",
    "## Résultats\n",
    "- l'outil fonctionne correctement (pas d'erreur identifiées, les temps de réponse restent néanmoins à améliorer).\n",
    "- l'analyse identifie des données anormales qui seraient à corriger (ou à expliquer) -> cf exemple dans la dernière cellule\n",
    "- le gain en taille de fichier varie de 50% (format texte non optimisé) à 80% (taille divisée par 5 !) dans un format optimisé et binaire. Ce niveau d'optimisation est tout à fait notable.\n",
    "- l'analyse permet de (re)trouver la logique de dépendance entre les colonnes qui minimise les incohérences\n",
    "- elle permet également de vérifier que les données respectent bien une structure imposée\n",
    "\n",
    "## Usages possibles \n",
    "- les indicateurs utilisés permettent de qualifier le fichier csv et sont déployables simplement:\n",
    "    - niveau de duplication des données\n",
    "    - lien de dépendance entre les colonnes (permet un codage réduit des données).\n",
    "- l'usage de format de données moins gourmand (sans dégradation des données) peut également être intéressant à déployer\n",
    "- les schémas de données pourraient intégrer cette dépendance entre colonnes (ex. s'il y a une colonne \"mois\" et une colonne \"trimestre\", on peut indiquer que la colonne \"trimestre\" est \"dérivée\" de la colonne \"mois\", ou bien si on a une colonne \"nom prénom\" et une colonne \"n° sécurité sociale\", on peut indiquer que les deux colonnes sont \"couplées\". \n",
    "- la qualité des données peut faire l'objet d'un indicateur qui mesure l'écart (existant / attendu) des relations entre colonnes\n",
    "- l'identification des enregistrements ne respectant pas une structure imposée permet de les éliminer ou de les corriger\n",
    "\n",
    "## Autres points\n",
    "- capacité ok de l'objet Iindexset à traiter des structures de données importantes (50 colonnes)\n",
    "- indicateurs et fonctions de conversion pertinentes\n",
    "- temps de réponse de la fonction 'coupling' à regarder\n",
    "- tests complémentaires à effectuer sur d'autres jeux de données\n",
    "- représentation graphique de la structure des données à regarder\n",
    "- indicateur qualité à regarder\n",
    "\n",
    "données utilisées : https://files.data.gouv.fr/lcsqa/concentrations-de-polluants-atmospheriques-reglementes/temps-reel/2022/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Initialisation\n",
    "- lecture des fichiers de 01/2022 issus de l'api (un fichier par jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size :  11132340\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "#os.chdir('C:/Users/a179227/OneDrive - Alliance/perso Wx/ES standard/python ESstandard/ES')\n",
    "from util import util\n",
    "from observation import Ilist, Iindex\n",
    "from copy import copy\n",
    "\n",
    "chemin = 'C:/Users/a179227/OneDrive - Alliance/perso Wx/ES standard/Environnemental-Sensing/python/validation/air/data_lcsqa/'\n",
    "file = chemin + 'FR_E2_2022-01-01.csv'\n",
    "\n",
    "print('file size : ', os.stat(file).st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2 197455 ['Date de début', 'Date de fin', 'Organisme', 'code zas', 'Zas', 'code site', 'nom site', \"type d'implantation\", 'Polluant', \"type d'influence\", 'discriminant', 'Réglementaire', \"type d'évaluation\", 'procédure de mesure', 'type de valeur', 'valeur', 'valeur brute', 'unité de mesure', 'taux de saisie', 'couverture temporelle', 'couverture de données', 'code qualité', 'validité'] 2.812849283218384\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = []\n",
    "t0 = time()\n",
    "annee = 2022\n",
    "mois = 1\n",
    "jour = 1\n",
    "for i in range(4):\n",
    "    file = chemin + 'FR_E2_' + str(annee) + '-' + format(mois, '02d') +'-' + format(jour+i, '02d') +'.csv'\n",
    "    data.append(pd.read_csv(file, sep=';'))\n",
    "data2 = pd.concat(data, ignore_index=True, join='inner').astype('category')\n",
    "\n",
    "print('data2', len(data2), list(data2), time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idxs (len, lenlidx, sumcodec) :  197455 23 13078 3.4289703369140625\n",
      "[96, 96, 18, 70, 70, 533, 533, 5, 9, 3, 26, 1, 4, 59, 2, 2926, 8616, 3, 1, 1, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "idxs2 = Ilist([Iindex(list(data2.loc[:,idx].cat.categories), idx, \n",
    "                     list(data2.loc[:,idx].cat.codes), lendefault=len(data2), castobj=False)\n",
    "              for idx in list(data2)])\n",
    "print('idxs (len, lenlidx, sumcodec) : ', len(idxs2), len(idxs2.idxlen), sum(idxs2.idxlen), time()-t0)\n",
    "t0=time()\n",
    "idxs.delindex('valeur brute')\n",
    "idxs.setvar('valeur')\n",
    "fullsize = len(idxs.to_obj(encoded=True, modecodec='full'))\n",
    "print('fullsize', fullsize, time()-t0)\n",
    "t0=time()\n",
    "minsize = len(idxs.to_obj(encoded=True, modecodec='nokeys'))\n",
    "print('minsize', minsize, time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.296959400177002\n",
      "4.681127071380615\n",
      "0.32598447799682617\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "for idx in idxs2.lindex:\n",
    "    val = idx.values\n",
    "print(time()-t0)\n",
    "pdkeys=[]\n",
    "for idx in idxs2.lindex:\n",
    "    pdkeys.append(pd.Series(idx.keys, dtype='category'))\n",
    "t0=time()\n",
    "for idx, keys in zip(idxs2.lindex,pdkeys):\n",
    "    #val = list(pd.DataFrame(keys.cat.rename_categories(zip(idx.codec, range(len(idx.codec)))).tolist())[0])\n",
    "    val = list(pd.DataFrame(keys.cat.rename_categories(pd.Series(zip(idx.codec, range(len(idx.codec))))).tolist())[0])\n",
    "    #val = idx.values\n",
    "print(time()-t0)\n",
    "t0=time()\n",
    "for idx, keys in zip(idxs2.lindex,pdkeys):\n",
    "    val = list(keys.cat.rename_categories(idx.codec))\n",
    "    #val = idx.values\n",
    "print(time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 197455 4.903850555419922\n"
     ]
    }
   ],
   "source": [
    "data1=[]\n",
    "t0 = time()\n",
    "annee = 2022\n",
    "mois = 1\n",
    "jour = 1\n",
    "for i in range(4):\n",
    "    file = chemin + 'FR_E2_' + str(annee) + '-' + format(mois, '02d') +'-' + format(jour+i, '02d') +'.csv'\n",
    "    with open(file, newline='', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=';')\n",
    "        names = next(reader)\n",
    "        data = []\n",
    "        for row in reader: data.append(row)\n",
    "    data1 += data\n",
    "data2 = util.list(list(zip(*data1)))\n",
    "print('data', len(data2[0]), time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## initialisation de l'objet Ilist\n",
    "- l'initialisation pourrait être automatisée à partir du fichier csv\n",
    "- identification de 64 775 valeurs différentes sur un total de 11 163 x 49 valeurs (\"taux d'unicité\" de 12%)\n",
    "- la taille minimale serait de 1,4 Mo (données csv \"quotées\") pour un maximum de 9,6 Mo (données csv \"quotées\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idxs (len, lenlidx, sumcodec) :  197455 23 13081 77.94894552230835\n",
      "fullsize 58218429 54.055302143096924\n",
      "minsize 1454420 2.4215452671051025\n"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "idxs = Ilist.ext(data2, names)\n",
    "print('idxs (len, lenlidx, sumcodec) : ', len(idxs), len(idxs.idxlen), sum(idxs.idxlen), time()-t0)\n",
    "t0=time()\n",
    "idxs.delindex('valeur brute')\n",
    "idxs.setvar('valeur')\n",
    "fullsize = len(idxs.to_obj(encoded=True, modecodec='full'))\n",
    "print('fullsize', fullsize, time()-t0)\n",
    "t0=time()\n",
    "minsize = len(idxs.to_obj(encoded=True, modecodec='nokeys'))\n",
    "print('minsize', minsize, time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 96, 18, 70, 70, 533, 533, 5, 9, 3, 27, 1, 4, 59, 2, 3, 1, 1, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "print(idxs.idxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## format non optimisé\n",
    "- le \"taux d'unicité\" reste à 12% (pas de modification des index)\n",
    "- le \"taux de codage\" est de 30% (remplacement des données dupliquées par un entier)\n",
    "- le gain de taille de fichier par rapport à un fichier \"quoté\" est de 61%\n",
    "- l'analyse de la structure montre que les données sont principalement du type \"linked\" (non ou peu structuré)\n",
    "- quelques colonnes sont de type \"derived\". Par exemple les index longitude(43) et latitude(44) sont bien dérivés de l'index coordonneesXY(13)\n",
    "- le taux de couplage (\"linkrate\") pour chacun des index est très proche de 0, ce qui signifie que les données devraient être de type \"derived\" (lien de dépendance par exemple comme entre les trimestres et les mois) ou \"coupled\" (lien biunivoque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "defaultsize = len(idxs.to_obj(encoded=True, modecodec='default'))\n",
    "print('defaultsize', defaultsize, time()-t0)\n",
    "print('indicator default : ', idxs.indicator(fullsize, defaultsize))\n",
    "pprint(idxs.indexinfos(keys=['num', 'name', 'lencodec', 'parent', 'typecoupl']), width=120)\n",
    "pprint(idxs.indexinfos(keys=['linkrate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Format optimisé\n",
    "- le \"taux d'unicité\" se dégrade légèrement (passage de 11,6% à 12,1%) par l'ajout d'index supplémentaires\n",
    "- le \"taux de codage\" par contre passe de 30% à 16% de par l'optimisation \n",
    "- le gain de taille de fichier par rapport à un fichier \"quoté\" est maintenant de 74%\n",
    "- l'utilisation d'un format binaire (codage CBOR pour Concise Binary Object Representation RFC 8949) permet d'améliorer encore le gain de taille de fichier (82%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#idxs.setcanonorder().sort()\n",
    "t0=time()\n",
    "optimizesize = len(idxs.to_obj(modecodec='optimize', encoded=True))\n",
    "print('optimizesize ', optimizesize, time()-t0)\n",
    "print('indicator optimize : ', idxs.indicator(fullsize, optimizesize))\n",
    "t0=time()\n",
    "js = idxs.to_obj(encoded=True, modecodec='optimize', encode_format='cbor')\n",
    "cborsize = len(js)\n",
    "print('cborsize', cborsize, time()-t0)\n",
    "print('indicator cbor : ', idxs.indicator(fullsize, cborsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Intégrité\n",
    "- la transformation inverse des données binaires permet de vérifier qu'on retombe bien sur les mêmes données (pas de dégradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0=time()\n",
    "idxs2 = Ilist.from_obj(js)\n",
    "print('fromcbor', len(idxs2), time()-t0)\n",
    "t0=time()\n",
    "verif = idxs2 == idxs\n",
    "print('controle égalité :', verif, time()-t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Exemple de données anormales\n",
    "- l'index coordonneesXY(13) est lié à l'index nom_station(9) avec un taux de couplage très faible, par ailleurs, le nombre de valeurs de ces deux index sont très proches (4503 nom_station pour 4459 coordonneesXY), ce qui signifie que dans la majorité des cas, on associe de façon unique une station et une position\n",
    "- les exemples ci-dessous montrent les écarts les plus important :\n",
    "    - la position [1.106329, 49.474202] est associée à 10 stations\n",
    "    - la station Camping Arinella est associée à 5 positions\n",
    "    \n",
    "- de même pour l'index coordonneesXY(13) qui est lié à l'index adresse_station(11), le taux de couplage est très faible avec 44 enregistrements sur 4456 en écart. On a par exemple quatre enregistrements avec la position (6.3491347, 47.3517596) et des adresses différentes ('58 Avenue du PrÃ©sident Kennedy 26', '58 Avenue du PrÃ©sident Kennedy 28', '58 Avenue du PrÃ©sident Kennedy 27', '58 Avenue du PrÃ©sident Kennedy 25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "champ = idxs.lindex\n",
    "print('Couplage entre ', champ[13].name, ' et ', champ[9].name, ' : ', champ[13].couplinginfos(champ[9])['typecoupl'])\n",
    "infosdefault = champ[9].couplinginfos(champ[13], default=True)\n",
    "print('Ecart : ', infosdefault['disttomin'], 'positions sur ', infosdefault['distmin'], '\\n') # moins de 1%\n",
    "nom_station = champ[9].tostdcodec(full=False)\n",
    "coordonneesXY = champ[13].tostdcodec(full=False) \n",
    "coordonneesXY.coupling(nom_station)\n",
    "c = Counter(coordonneesXY.codec).most_common(5)\n",
    "print('les 5 positions avec le plus de stations: ', c, '\\n')\n",
    "print('liste des stations associées à la position', c[0][0], ' : ', \n",
    "      set([nom_station[i] for i in coordonneesXY.recordfromvalue(c[0][0])]), '\\n')\n",
    "coordonneesXY = champ[13].tostdcodec(full=False)\n",
    "nom_station.coupling(coordonneesXY)\n",
    "c = Counter(nom_station.codec).most_common(5)\n",
    "print('les 5 stations avec le plus de positions: ', c, '\\n')\n",
    "print('liste des positions associées à la station', c[0][0], ' : ', \n",
    "      set([coordonneesXY[i] for i in nom_station.recordfromvalue(c[0][0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Couplage entre ', champ[11].name, ' et ', champ[13].name, ' : ', champ[11].couplinginfos(champ[13])['typecoupl'])\n",
    "infosdefault = champ[13].couplinginfos(champ[11], default=True)\n",
    "print('Ecart : ', infosdefault['disttomin'], 'positions sur ', infosdefault['distmin'], '\\n') # moins de 1%\n",
    "coordonneesXY = champ[13].tostdcodec(full=False)\n",
    "adresse_station = champ[11].tostdcodec(full=False) \n",
    "adresse_station.coupling(coordonneesXY)\n",
    "c = Counter(adresse_station.codec).most_common(5)\n",
    "print('les 5 adresses avec le plus de positions : ', c, '\\n')\n",
    "print('liste des position associées à l adresse :', c[0][0], ' : ', \n",
    "      set([coordonneesXY[i] for i in adresse_station.recordfromvalue(c[0][0])]), '\\n')\n",
    "adresse_station = champ[11].tostdcodec(full=False)\n",
    "coordonneesXY.coupling(adresse_station)\n",
    "c = Counter(coordonneesXY.codec).most_common(5)\n",
    "print('les 5 positions avec le plus d adresses: ', c, '\\n')\n",
    "print('liste des adresses associées à la position', c[0][0], ' : ', \n",
    "      set([adresse_station[i] for i in coordonneesXY.recordfromvalue(c[0][0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Détection des incohérences de relations entre champs\n",
    "- la fonction coupling permet de réorganiser la structure des relations en isolant les enregistrements incohérents\n",
    "- dans le cas ci-dessous, elle est appliquée de façon automatique (minimisation des incohérences)\n",
    "- les incohérences minimales sont dans le cas présent de 12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idxs3 = Ilist.from_obj(js)\n",
    "idxs3.coupling()\n",
    "duplic = idxs3.getduplicates(resindex=ES.filter)\n",
    "#duplic = idxs3.getduplicates(idxs3.lname, ES.filter)\n",
    "print('nombre d enregistrements incohérents : ', len(duplic))\n",
    "idxs3.applyfilter()\n",
    "print('nombre d enregistrements cohérents et liste des indexs non dérivés : ', len(idxs3), idxs3.primary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Vérification simple\n",
    "- on peut vérifier par exemple qu'une position est associée à une unique station et que réciproquement chaque station n'a qu'une seule position (relation 1-1 entre les deux champs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs4 = copy(idxs2)\n",
    "champ = idxs4.lindex\n",
    "notcoupl = champ[13].coupling(champ[9], derived=False)\n",
    "print('nombre de pdc avec position/station non couplées : ', len(notcoupl))\n",
    "print('\\nliste des premières incohérences : ')\n",
    "liste = []\n",
    "for i in range(100): liste.append((champ[9][notcoupl[i]], champ[13][notcoupl[i]]))\n",
    "pprint(set(liste), width=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vérification par rapport à une structure imposée\n",
    "- une autre utilisation possible est de vérifier les données par rapport à un modèle de données défini\n",
    "- dans cet exemple, on peut regrouper les colonnes suivant quatre entités (ceci revient à considérer les colonnes comme des attributs de chacune des entités) : les opérateurs (colonne 6), les aménageurs (colonne 1), les stations (colonne 9), les pdc (colonne 15) -> cf quatre premières lignes ci-dessous\n",
    "- on peut également indiquer les dépendances entre les quatre entités (les opérateurs et aménageurs sont dérivés par rapport aux stations qui elles sont dérivées par rapport aux pdc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "row_operateur   = [6,3,4,5]\n",
    "row_amenageur   = [1,0,2]\n",
    "row_station     = [9,10,11,12,13,14,23,24,25,26,27,28,29,30,31,32,33,34,35,36,\n",
    "                   43,44,45,46,47,48]\n",
    "row_pdc         = [15,16,17,18,19,20,21,22,7,8,37,38,39,40,41,42]\n",
    "\n",
    "champ = idxs2.lindex\n",
    "operateur       = [champ[i] for i in row_operateur]\n",
    "amenageur       = [champ[i] for i in row_amenageur]\n",
    "station         = [champ[i] for i in row_station]\n",
    "pdc             = [champ[i] for i in row_pdc]\n",
    "\n",
    "operateur[0].coupling(operateur[1:])\n",
    "amenageur[0].coupling(amenageur[1:])\n",
    "station  [0].coupling(station  [1:])\n",
    "pdc      [0].coupling(pdc      [1:])\n",
    "\n",
    "station  [0].coupling([operateur[0], amenageur[0]])\n",
    "pdc      [0].coupling(station[0])\n",
    "pprint(idxs2.indexinfos(keys=['num', 'name', 'parent', 'typecoupl']), width=120)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en cohérence des données\n",
    "- l'application de la structure imposée permet d'identifie les enregistrements qui ne respectent pas la structure (cf exemples indiqués plus haut).\n",
    "- dans l'exemple proposé, on identifie 48% des données ne respectant pas la structure\n",
    "- on vérifie également qu'avec les données restantes, la structure est bien respectée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "duplic = idxs2.getduplicates(idxs2.lname, ES.filter)\n",
    "print(Counter(idxs2.lidx[49].keys))\n",
    "idxs2.applyfilter()\n",
    "idxs2.to_csv()\n",
    "print(len(idxs2), idxs2.primary)\n",
    "pprint(idxs2.indexinfos(keys=['num', 'name', 'parent', 'typecoupl']), width=120)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
