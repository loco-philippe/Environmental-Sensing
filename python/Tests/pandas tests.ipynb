{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objet : Test pandas\n",
    "\n",
    "## Objectif\n",
    "\n",
    "- list vers pandas series\n",
    "\n",
    "## Résultats\n",
    "- à préciser\n",
    "\n",
    "## Usages possibles \n",
    "- rempacement _keys et _keys par Series\n",
    "\n",
    "## Autres points\n",
    "- à tester\n",
    "\n",
    "données utilisées : https://files.data.gouv.fr/lcsqa/concentrations-de-polluants-atmospheriques-reglementes/temps-reel/2022/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1]\n",
      "[1, 0, 0, 1, 1, 0]\n",
      "[0 1 0 1]\n",
      "[1 0 0 1 1 0]\n",
      "[0 1 0 1]\n",
      "[1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# derived\n",
    "\n",
    "# to_json\n",
    "#keys = idx.derkeys(self.lindex[parent])\n",
    "\n",
    "def derkeys(self_keys, parent_keys):\n",
    "        '''return keys derived from parent keys\n",
    "\n",
    "        *Parameters*\n",
    "\n",
    "        - **parent** : Field - parent\n",
    "\n",
    "        *Returns* : list of keys'''\n",
    "        derkey = [-1] * (max(parent_keys)+1)\n",
    "        for i in range(len(self_keys)):\n",
    "            derkey[parent_keys[i]] = self_keys[i]\n",
    "        if min(derkey) < 0:\n",
    "            raise FieldError(\"parent is not a derive Field\")\n",
    "        return derkey\n",
    "\n",
    "def keysfromderkeys(parentkeys, derkeys):\n",
    "        '''return keys from parent keys and derkeys\n",
    "\n",
    "        *Parameters*\n",
    "\n",
    "        - **parentkeys** : list of keys from parent\n",
    "        - **derkeys** : list of derived keys\n",
    "\n",
    "        *Returns* : list of keys'''\n",
    "        return [derkeys[pkey] for pkey in parentkeys]\n",
    "\n",
    "parent_keys = [1, 2, 2, 3, 3, 0]\n",
    "child_keys  = [1, 0, 0, 1, 1, 0]\n",
    "der_keys = derkeys(child_keys, parent_keys)\n",
    "print(der_keys)\n",
    "print(keysfromderkeys(parent_keys, der_keys))\n",
    "\n",
    "def np_derkeys(self_keys, parent_keys):\n",
    "        '''return keys derived from parent keys\n",
    "\n",
    "        *Parameters*\n",
    "\n",
    "        - **parent** : Field - parent\n",
    "\n",
    "        *Returns* : list of keys'''\n",
    "        derkey = np.full([max(parent_keys)+1], -1)\n",
    "        derkey[parent_keys] = self_keys\n",
    "        if min(derkey) < 0:\n",
    "            raise FieldError(\"parent is not a derive Field\")\n",
    "        return derkey\n",
    "\n",
    "def np_keysfromderkeys(parentkeys, derkeys):\n",
    "        '''return keys from parent keys and derkeys\n",
    "\n",
    "        *Parameters*\n",
    "\n",
    "        - **parentkeys** : list of keys from parent\n",
    "        - **derkeys** : list of derived keys\n",
    "\n",
    "        *Returns* : list of keys'''\n",
    "        return derkeys[parentkeys]\n",
    "    \n",
    "np_der_keys = np_derkeys(child_keys, parent_keys)\n",
    "print(np_der_keys)\n",
    "print(np_keysfromderkeys(parent_keys, np_der_keys))\n",
    "\n",
    "parent_keys = np.array([1, 2, 2, 3, 3, 0])\n",
    "child_keys  = np.array([1, 0, 0, 1, 1, 0])\n",
    "np_der_keys = np_derkeys(child_keys, parent_keys)\n",
    "print(np_der_keys)\n",
    "print(np_keysfromderkeys(parent_keys, np_der_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 20 30 40 50] [0 1 2 2 3 2 4 4 2 0] [2 1 4 1 2]\n",
      "[[10, 20, 30, 40, 50], [0, 1, 3, 4, 4, 0, 2]]\n",
      "[10, 20, 40, 50, 50, 10, 30]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from ntv_numpy import Dcomplete, Dfull\n",
    "\n",
    "data = np.array([10, 20, 30, 30, 40, 30, 50, 50, 30, 10])\n",
    "\n",
    "values, cat, count = np.unique(data, return_inverse=True, return_counts=True)\n",
    "print(values, cat, count)\n",
    "values, idx, cat, count = np.unique(data, return_inverse=True, return_counts=True, return_index=True)\n",
    "idx_fill = list(count).index(max(count))\n",
    "#fill_value = values[idx_fill]\n",
    "sp_index = [row for row, cat in zip(range(len(cat)), cat) if cat != idx_fill] + [idx_fill]\n",
    "sp_values = list(data[sp_index])\n",
    "sp_index[-1]= -1\n",
    "#print(sp_index)\n",
    "#print(sp_values)\n",
    "print(Dcomplete(sp_values).to_json())\n",
    "print(Dfull(sp_values).to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ntv_numpy import Dcomplete\n",
    "cp_values = Dcomplete(sp_values)\n",
    "print(cp_values.coding)\n",
    "print(cp_values.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.arrays import SparseArray\n",
    "arr = SparseArray(data, dtype=pd.SparseDtype(\"int\", fill_value))\n",
    "print(list(arr.sp_index.indices))\n",
    "print(list(arr.sp_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.arrays import SparseArray\n",
    "arr = SparseArray([0, 0, 10, 20, 0, 0, 10, 30])\n",
    "# attributs : sp_values, sp_index, fill_values\n",
    "print(list(arr.sp_index.indices))\n",
    "print(list(arr.sp_values))\n",
    "index = list(arr.sp_index.indices)\n",
    "val = list(arr.sp_values)\n",
    "values = [0] * 8\n",
    "for ind, idx in enumerate(arr.sp_index.indices):\n",
    "    values[idx] = arr.sp_values[ind]\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pour les tuples\n",
    "dist = [[len(set(zip(keys[i], keys[j]))) # 9.3\n",
    "dist = [[len(pd.Series(zip(keys[keys.columns[i]], keys[keys.columns[j]])).astype('category').cat.categories) # 65\n",
    "dist = [[len(np.unique(np.column_stack((keys[i], keys[j])), axis=0)) # 73.7\n",
    "dist = [[len(np.unique(np.fromiter(zip(keys[i], keys[j]), dtype='object'))) #146\n",
    "dist = [[len(keys[[keys.columns[i], keys.columns[j]]].apply(tuple, axis=1).astype('category').cat.categories) # 580"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import shapely\n",
    "from shapely.geometry import Point, Polygon\n",
    "import pandas as pd\n",
    "from json_ntv import ShapelyConnec\n",
    "\n",
    "data =pd.Series([Point(1,2), Point(1,3), Point(2,4), Polygon([[1.0, 2.0], [1.0, 3.0], [2.0, 4.0]]),\n",
    "Polygon([[1.0, 2.0], [1.0, 30.0], [30.0, 30.0], [30,2]], [[[5.0, 16.0], [5.0, 27.0], [20.0, 27.0]]])])\n",
    "\n",
    "coor = data.apply(ShapelyConnec.to_coord)\n",
    "print(coor)\n",
    "print(coor.to_list())\n",
    "data2 = coor.apply(ShapelyConnec.to_geometry)\n",
    "print(data2)\n",
    "\n",
    "coor2 = data.apply(shapely.geometry.mapping)\n",
    "print(coor2)\n",
    "\n",
    "coor3 = coor2.apply(json.dumps)\n",
    "print(coor3)\n",
    "\n",
    "coor4 = coor3.apply(json.loads)\n",
    "print(coor4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor5 = data.apply(shapely.to_geojson)\n",
    "coor5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "styler = df.style.highlight_max()\n",
    "\n",
    "styler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styler.to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#styler.to_json() -> ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "coef = 40\n",
    "period = 20\n",
    "repeat = 50\n",
    "leng = coef * period * repeat\n",
    "lseq = coef * period\n",
    "tn = 0\n",
    "tp = 0\n",
    "for i in range(50):\n",
    "    t0 =time()\n",
    "    seq = np.stack(tuple([np.arange(period)] * coef), axis=1).reshape(lseq)\n",
    "    keys1 = list(np.stack(tuple([seq] * repeat)).reshape(leng))\n",
    "    tn += time()-t0\n",
    "\n",
    "    t0 =time()\n",
    "    keys2 = [(i % lseq) // coef for i in range(leng)]\n",
    "    tp += time()-t0\n",
    "print(tn/10, tp/10, tp/tn, keys1 == keys2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "## Initialisation\n",
    "- lecture des fichiers de 01/2022 issus de l'api (un fichier par jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "#os.chdir('C:/Users/a179227/OneDrive - Alliance/perso Wx/ES standard/python ESstandard/ES')\n",
    "from util import util\n",
    "from observation import Ilist, Iindex\n",
    "from copy import copy\n",
    "\n",
    "chemin='C:/Users/a179227/OneDrive - Alliance/perso Wx/ES standard/Environnemental-Sensing/python/validation/air/data_lcsqa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.Series([date(2021,1,1), date(2022, 1, 2)])\n",
    "print(sr)\n",
    "sr2 = sr.astype(str) # conversion date -> str\n",
    "js = sr2.to_json(orient='records', date_format='iso', default_handler=str)\n",
    "print(js)\n",
    "\n",
    "sr3 = pd.read_json(js, typ='series')\n",
    "sr4 = pd.to_datetime(sr3).dt.date # conversion str -> date\n",
    "print(sr4.equals(sr))\n",
    "print(sr4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "t0 = time()\n",
    "annee = 2022\n",
    "mois = 1\n",
    "jour = 1\n",
    "for i in range(4):\n",
    "    file = chemin + 'FR_E2_' + str(annee) + '-' + format(mois, '02d') +'-' + format(jour+i, '02d') +'.csv'\n",
    "    data.append(pd.read_csv(file, sep=';'))\n",
    "data2 = pd.concat(data, ignore_index=True, join='inner').astype('category')\n",
    "data2.pop('valeur brute')\n",
    "print('data2', len(data2), list(data2), time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "idxs2 = Ilist.obj(data2)\n",
    "print('idxs (len, lenlidx, sumcodec) : ', len(idxs2), len(idxs2.idxlen), sum(idxs2.idxlen), time()-t0)\n",
    "#idxs2.delindex('valeur brute')\n",
    "idxs2.setvar('valeur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#calcul de values à partir de keys et codec\n",
    "t0=time()\n",
    "for idx in idxs2.lindex:\n",
    "    val = idx.values\n",
    "print('list to val old: ', time()-t0)\n",
    "#génération d'une liste de Series à partir d'une liste de codec\n",
    "t0=time()\n",
    "pdcodec=[pd.Series(idx.codec) for idx in idxs2.lindex]\n",
    "print('list to codec : ', time()-t0)#génération d'une liste de Series à partir d'une liste de keys\n",
    "#génération d'une liste de Series simple à partir d'une liste de keys\n",
    "t0=time()\n",
    "pdkeyss=[pd.Series(idx.keys, dtype='int64') for idx in idxs2.lindex]\n",
    "print('list to cat simple : ', time()-t0)\n",
    "#génération d'une liste de Series à partir d'une liste de keys\n",
    "t0=time()\n",
    "pdkeys=[pd.Series(idx.keys, dtype='category') for idx in idxs2.lindex]\n",
    "print('list to cat : ', time()-t0)\n",
    "#génération d'une liste de Series à partir d'une liste de keys\n",
    "pdkeys=[]\n",
    "t0=time()\n",
    "for idx in idxs2.lindex:\n",
    "    pdkeys.append(pd.Series(idx.keys, dtype='category'))\n",
    "print('list to cat : ', time()-t0)#calcul de values à partir de keys et codec avec une Series categorical (default)\n",
    "t0=time()\n",
    "for idx in idxs2.lindex:\n",
    "    val = list(pd.Series(idx.keys, dtype='category').cat.rename_categories(idx.codec))\n",
    "print('list to val (def): ', time()-t0)\n",
    "#calcul de values à partir de keys et codec avec une Series map (all)\n",
    "t0=time()\n",
    "for idx in idxs2.lindex:\n",
    "    val = list(pd.Series(idx.keys).map(pd.Series(idx.codec)))\n",
    "print('list to val (all map): ', time()-t0)\n",
    "#calcul de values à partir de keys et codec avec une Series (quelconque)\n",
    "t0=time()\n",
    "for idx, keys in zip(idxs2.lindex,pdkeys):\n",
    "    val = list(pd.DataFrame(keys.cat.rename_categories(pd.Series(zip(idx.codec, range(len(idx.codec))))).tolist())[0])\n",
    "print('list to val (all tuple): ', time()-t0)\n",
    "#calcul de values à partir d'une Series de keys et codec (default)\n",
    "t0=time()\n",
    "for idx, keys in zip(idxs2.lindex,pdkeys):\n",
    "    val = list(keys.cat.rename_categories(idx.codec))\n",
    "print('cat to val (def): ', time()-t0)\n",
    "#calcul de values à partir d'une Series de keys et codec avec une Series (quelconque)\n",
    "t0=time()\n",
    "for idx, keys in zip(idxs2.lindex,pdkeys):\n",
    "    val = list(keys.map(pd.Series(idx.codec)))\n",
    "print('cat to val (all): ', time()-t0)\n",
    "#calcul de values à partir d'une Series de keys et une series codec (quelconque)\n",
    "t0=time()\n",
    "for cod, keys in zip(pdcodec,pdkeyss):\n",
    "    val = list(keys.map(cod))\n",
    "print('simple to val (all series simple): ', time()-t0)\n",
    "#calcul de values à partir d'une Series de keys et une series codec (quelconque)\n",
    "t0=time()\n",
    "for cod, keys in zip(pdcodec,pdkeys):\n",
    "    val = list(keys.map(cod))\n",
    "print('cat to val (all series): ', time()-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(val[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESValue\n",
    "from observation import DatationValue, NamedValue\n",
    "ser = pd.Series([DatationValue('date'), DatationValue('date2'), DatationValue('date'), NamedValue(3,'test')], dtype='category')\n",
    "print(ser.cat.categories, '\\n', list(ser.cat.codes))\n",
    "ser2 = pd.Series([0,11,0,22]).astype('category')\n",
    "serbis = ser2.cat.rename_categories([DatationValue('date'), DatationValue('date2'),  NamedValue(3,'test')])\n",
    "min(ser.astype('object') == serbis.astype('object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation dataframe à partir de Ilist\n",
    "ilm = Ilist.obj([['plants', ['fruit', 'fruit', 'fruit', 'fruit', 'vegetable', 'vegetable', 'vegetable', 'fruit']],\n",
    "                  ['quantity', ['kg', '10 kg', 'kg', '10 kg', 'kg', '10 kg', 'kg', '10 kg']],\n",
    "                  ['product', ['apple', 'apple', 'orange', 'orange', 'peppers', 'peppers', 'banana', 'banana']],\n",
    "                  ['price', [1, 10, 2, 20, 1.5, 15, 0.5, 5], -1]])\n",
    "df = pd.concat([idx.to_pandas(index=False, series=False).astype('category') for idx in ilm.lindex], axis=1)\n",
    "col = pd.MultiIndex.from_arrays([ilm.lname, ilm.lisvar], names=['name', 'var'])\n",
    "df.columns = col\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction append\n",
    "s2 = pd.Series(['fruit', 'kg', 'banana', 7], index=df.columns)\n",
    "df = pd.concat([df, s2.to_frame().T], ignore_index=True).astype('category') \n",
    "ds = df['price'].squeeze()\n",
    "ds.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.Series(['b', 'a', 'a', 'd']).astype('category')\n",
    "b = pd.Series([1,2,4,3]).astype('category')\n",
    "coup = pd.Series(list(zip(*(list(a), list(b))))).astype('category')\n",
    "print(coup)\n",
    "\n",
    "print(a.cat.codes)\n",
    "print(coup.cat.codes)\n",
    "res = coup.cat.codes == a.cat.codes\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([0,1,2,3,2]).astype('category')\n",
    "codec = ['a', 'b', 'c', 'b']\n",
    "print(a.cat.codes.map(pd.Series(codec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtrage suivant var\n",
    "df.loc[:,(slice(None),False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = [[name, list(ds)] for name,ds in df.items()]\n",
    "print(full, '\\n')\n",
    "default = [[name, list(ds.cat.categories), list(ds.cat.codes)] for name,ds in df.astype('category').items()]\n",
    "print(default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"one\": [1.0, 2.0, 3.0, 4.0], \"two\": [4.0, 3.0, 2.0, 1.0]}\n",
    "df = pd.DataFrame(d)\n",
    "df[\"three\"] = True\n",
    "df[\"four\"] = [True, True, False, False]\n",
    "df[\"total\"] = True\n",
    "df['somme'] = 0\n",
    "f =[1,3]\n",
    "df.loc[f, \"three\"] = False\n",
    "df['total'] = df['total'] & df['three']\n",
    "df['total'] = df['total'] & df['four']\n",
    "df['somme'] = df['somme'] - df['three'] +1\n",
    "df['somme'] = df['somme'] - df['four'] +1\n",
    "print(df)\n",
    "print(df.four.sum(), df.total.sum())\n",
    "maxi = max(df.somme) \n",
    "df_max = df[df.somme == maxi]\n",
    "print(maxi, len(df_max))\n",
    "print(df_max.loc[:,('one','three', 'four', 'total')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ilm.to_obj(modecodec='full'), '\\n')\n",
    "print(ilm.to_obj(modecodec='default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generation de json (full et default)\n",
    "import json\n",
    "t0=time()\n",
    "full = json.dumps([[name, list(ds)] for name,ds in data2.items()])\n",
    "print('fullsize', len(full), time()-t0)\n",
    "t0=time()\n",
    "default = json.dumps([[name, list(ds.cat.categories), list(ds.cat.codes)] for name,ds in data2.astype('category').items()])\n",
    "print('defaultsize', len(default), time()-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "fullsize = len(idxs2.to_obj(encoded=True, modecodec='full'))\n",
    "print('fullsize', fullsize, time()-t0)\n",
    "t0=time()\n",
    "minsize = len(idxs2.to_obj(encoded=True, modecodec='nokeys'))\n",
    "print('minsize', minsize, time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## format non optimisé\n",
    "- le \"taux d'unicité\" reste à 12% (pas de modification des index)\n",
    "- le \"taux de codage\" est de 30% (remplacement des données dupliquées par un entier)\n",
    "- le gain de taille de fichier par rapport à un fichier \"quoté\" est de 61%\n",
    "- l'analyse de la structure montre que les données sont principalement du type \"linked\" (non ou peu structuré)\n",
    "- quelques colonnes sont de type \"derived\". Par exemple les index longitude(43) et latitude(44) sont bien dérivés de l'index coordonneesXY(13)\n",
    "- le taux de couplage (\"linkrate\") pour chacun des index est très proche de 0, ce qui signifie que les données devraient être de type \"derived\" (lien de dépendance par exemple comme entre les trimestres et les mois) ou \"coupled\" (lien biunivoque)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "defaultsize = len(idxs2.to_obj(encoded=True, modecodec='default'))\n",
    "print('defaultsize', defaultsize, time()-t0)\n",
    "print('indicator default : ', idxs2.indicator(fullsize, defaultsize))\n",
    "pprint(idxs2.indexinfos(keys=['num', 'name', 'lencodec', 'parent', 'typecoupl']), width=120)\n",
    "pprint(idxs2.indexinfos(keys=['linkrate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Format optimisé\n",
    "- le \"taux d'unicité\" se dégrade légèrement (passage de 11,6% à 12,1%) par l'ajout d'index supplémentaires\n",
    "- le \"taux de codage\" par contre passe de 30% à 16% de par l'optimisation \n",
    "- le gain de taille de fichier par rapport à un fichier \"quoté\" est maintenant de 74%\n",
    "- l'utilisation d'un format binaire (codage CBOR pour Concise Binary Object Representation RFC 8949) permet d'améliorer encore le gain de taille de fichier (82%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idxs.setcanonorder().sort()\n",
    "t0=time()\n",
    "optimizesize = len(idxs2.to_obj(modecodec='optimize', encoded=True))\n",
    "print('optimizesize ', optimizesize, time()-t0)\n",
    "print('indicator optimize : ', idxs2.indicator(fullsize, optimizesize))\n",
    "t0=time()\n",
    "js = idxs2.to_obj(encoded=True, modecodec='optimize', format='cbor')\n",
    "cborsize = len(js)\n",
    "print('cborsize', cborsize, time()-t0)\n",
    "print('indicator cbor : ', idxs2.indicator(fullsize, cborsize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Intégrité\n",
    "- la transformation inverse des données binaires permet de vérifier qu'on retombe bien sur les mêmes données (pas de dégradation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=time()\n",
    "idxs3 = Ilist.from_obj(js)\n",
    "print('fromcbor', len(idxs3), time()-t0)\n",
    "t0=time()\n",
    "verif = idxs2 == idxs3\n",
    "print('controle égalité :', verif, time()-t0)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
